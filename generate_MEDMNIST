import numpy as np
import os
import random
import torch
import torchvision.transforms as transforms
from utils.dataset_utils import check, separate_data, split_data, save_file
from medmnist import (PathMNIST, ChestMNIST, DermaMNIST, OCTMNIST, PneumoniaMNIST,
                      RetinaMNIST, BreastMNIST, BloodMNIST, TissueMNIST, OrganAMNIST,
                      OrganCMNIST, OrganSMNIST)

# Set random seeds for reproducibility
random.seed(1)
np.random.seed(1)
torch.manual_seed(1)
import gc
import ujson
from sklearn.model_selection import train_test_split

batch_size = 10
train_ratio = 0.75 # merge original training set and test set, then split it manually. 
alpha = 0.1 # for Dirichlet distribution. 100 for exdir

def check(config_path, train_path, test_path, num_clients, niid=False, 
        balance=True, partition=None):
    # check existing dataset
    if os.path.exists(config_path):
        with open(config_path, 'r') as f:
            config = ujson.load(f)
        if config['num_clients'] == num_clients and \
            config['non_iid'] == niid and \
            config['balance'] == balance and \
            config['partition'] == partition and \
            config['alpha'] == alpha and \
            config['batch_size'] == batch_size:
            print("\nDataset already generated.\n")
            return True

    dir_path = os.path.dirname(train_path)
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)
    dir_path = os.path.dirname(test_path)
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)

    return False
def separate_data(data, num_clients, num_classes, niid=False, balance=False, partition=None, class_per_client=None):
    X = [[] for _ in range(num_clients)]
    y = [[] for _ in range(num_clients)]
    statistic = [[] for _ in range(num_clients)]

    dataset_content, dataset_label = data
    least_samples = int(min(batch_size / (1 - train_ratio), len(dataset_label) / num_clients / 2))

    dataidx_map = {}

    if not niid:
        partition = 'pat'
        class_per_client = num_classes

    if partition == 'pat':
        idxs = np.array(range(len(dataset_label)))
        idx_for_each_class = []
        for i in range(num_classes):
            idx_for_each_class.append(idxs[dataset_label == i])

        class_num_per_client = [class_per_client for _ in range(num_clients)]
        for i in range(num_classes):
            selected_clients = []
            for client in range(num_clients):
                if class_num_per_client[client] > 0:
                    selected_clients.append(client)
            selected_clients = selected_clients[:int(np.ceil((num_clients / num_classes) * class_per_client))]

            num_all_samples = len(idx_for_each_class[i])
            num_selected_clients = len(selected_clients)
            num_per = num_all_samples / num_selected_clients
            if balance:
                num_samples = [int(num_per) for _ in range(num_selected_clients - 1)]
            else:
                num_samples = np.random.randint(max(num_per / 10, least_samples / num_classes), num_per, num_selected_clients - 1).tolist()
            num_samples.append(num_all_samples - sum(num_samples))

            idx = 0
            for client, num_sample in zip(selected_clients, num_samples):
                if client not in dataidx_map.keys():
                    dataidx_map[client] = idx_for_each_class[i][idx:idx + num_sample]
                else:
                    dataidx_map[client] = np.append(dataidx_map[client], idx_for_each_class[i][idx:idx + num_sample], axis=0)
                idx += num_sample
                class_num_per_client[client] -= 1

    elif partition == "dir":
        min_size = 0
        K = num_classes
        N = len(dataset_label)

        try_cnt = 1
        while min_size < least_samples:
            if try_cnt > 1:
                print(f'Client data size does not meet the minimum requirement {least_samples}. Try allocating again for the {try_cnt}-th time.')

            idx_batch = [[] for _ in range(num_clients)]
            for k in range(K):
                idx_k = np.where(dataset_label == k)[0]
                np.random.shuffle(idx_k)
                proportions = np.random.dirichlet(np.repeat(alpha, num_clients))
                proportions = np.array([p * (len(idx_j) < N / num_clients) for p, idx_j in zip(proportions, idx_batch)])
                proportions = proportions / proportions.sum()
                proportions = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1]
                idx_batch = [idx_j + idx.tolist() for idx_j, idx in zip(idx_batch, np.split(idx_k, proportions))]
                min_size = min([len(idx_j) for idx_j in idx_batch])
            try_cnt += 1

        for j in range(num_clients):
            dataidx_map[j] = idx_batch[j]
    
    elif partition == 'exdir':
        C = class_per_client
        
        min_size_per_label = 0
        min_require_size_per_label = max(C * num_clients // num_classes // 2, 1)
        if min_require_size_per_label < 1:
            raise ValueError
        clientidx_map = {}
        while min_size_per_label < min_require_size_per_label:
            for k in range(num_classes):
                clientidx_map[k] = []
            for i in range(num_clients):
                labelidx = np.random.choice(range(num_classes), C, replace=False)
                for k in labelidx:
                    clientidx_map[k].append(i)
            min_size_per_label = min([len(clientidx_map[k]) for k in range(num_classes)])
        
        dataidx_map = {}
        y_train = dataset_label
        min_size = 0
        min_require_size = 10
        K = num_classes
        N = len(y_train)
        print("\n*****clientidx_map*****")
        print(clientidx_map)
        print("\n*****Number of clients per label*****")
        print([len(clientidx_map[i]) for i in range(len(clientidx_map))])

        while min_size < min_require_size:
            idx_batch = [[] for _ in range(num_clients)]
            for k in range(K):
                idx_k = np.where(y_train == k)[0]
                np.random.shuffle(idx_k)
                proportions = np.random.dirichlet(np.repeat(alpha, num_clients))
                proportions = np.array([p * (len(idx_j) < N / num_clients and j in clientidx_map[k]) for j, (p, idx_j) in enumerate(zip(proportions, idx_batch))])
                proportions = proportions / proportions.sum()
                proportions = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1]
                if proportions[-1] != len(idx_k):
                    for w in range(clientidx_map[k][-1], num_clients - 1):
                        proportions[w] = len(idx_k)
                idx_batch = [idx_j + idx.tolist() for idx_j, idx in zip(idx_batch, np.split(idx_k, proportions))]
                min_size = min([len(idx_j) for idx_j in idx_batch])

        for j in range(num_clients):
            np.random.shuffle(idx_batch[j])
            dataidx_map[j] = idx_batch[j]
    
    else:
        # Default partition method
        # Implement any additional partition logic here or raise an exception if needed
        print(f"Partition method '{partition}' not recognized or not implemented.")
        raise NotImplementedError(f"Partition method '{partition}' not implemented.")

    for client in range(num_clients):
        idxs = dataidx_map[client]
        X[client] = dataset_content[idxs]
        y[client] = dataset_label[idxs]

        for i in np.unique(y[client]):
            statistic[client].append((int(i), int(sum(y[client] == i))))
            
    del data
    for client in range(num_clients):
        print(f"Client {client}\t Size of data: {len(X[client])}\t Labels: ", np.unique(y[client]))
        print(f"\t\t Samples of labels: ", [i for i in statistic[client]])
        print("-" * 50)

    return X, y, statistic



def split_data(X, y):
    # Split dataset
    train_data, test_data = [], []
    num_samples = {'train':[], 'test':[]}

    for i in range(len(y)):
        X_train, X_test, y_train, y_test = train_test_split(
            X[i], y[i], train_size=train_ratio, shuffle=True)

        train_data.append({'x': X_train, 'y': y_train})
        num_samples['train'].append(len(y_train))
        test_data.append({'x': X_test, 'y': y_test})
        num_samples['test'].append(len(y_test))

    print("Total number of samples:", sum(num_samples['train'] + num_samples['test']))
    print("The number of train samples:", num_samples['train'])
    print("The number of test samples:", num_samples['test'])
    print()
    del X, y
    # gc.collect()

    return train_data, test_data

def save_file(config_path, train_path, test_path, train_data, test_data, num_clients, 
                num_classes, statistic, niid=False, balance=True, partition=None):
    config = {
        'num_clients': num_clients, 
        'num_classes': num_classes, 
        'non_iid': niid, 
        'balance': balance, 
        'partition': partition, 
        'Size of samples for labels in clients': statistic, 
        'alpha': alpha, 
        'batch_size': batch_size, 
    }

    # gc.collect()
    print("Saving to disk.\n")

    for idx, train_dict in enumerate(train_data):
        with open(train_path + str(idx) + '.npz', 'wb') as f:
            np.savez_compressed(f, data=train_dict)
    for idx, test_dict in enumerate(test_data):
        with open(test_path + str(idx) + '.npz', 'wb') as f:
            np.savez_compressed(f, data=test_dict)
    with open(config_path, 'w') as f:
        ujson.dump(config, f)

    print("Finish generating dataset.\n")


def generate_dataset(dataset_class, dir_path, num_clients, niid, balance, partition):
    # Create directory if it does not exist
    os.makedirs(dir_path, exist_ok=True)
    
    # Paths for configuration, training, and testing data
    config_path = os.path.join(dir_path, "config.json")
    train_path = os.path.join(dir_path, "train/")
    test_path = os.path.join(dir_path, "test/")

    # Check if dataset generation is needed
    if check(config_path, train_path, test_path, num_clients, niid, balance, partition):
        return

    # Transformation for the dataset
    transform = transforms.Compose([
        transforms.Grayscale(),
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5])
    ])

    # Load data
    trainset = dataset_class(split='train', transform=transform, download=True)
    testset = dataset_class(split='test', transform=transform, download=True)
    
    # Combine train and test datasets
    dataset_image = np.concatenate((trainset.imgs, testset.imgs), axis=0)
    dataset_label = np.concatenate((trainset.labels, testset.labels), axis=0)

    # Normalize images to [0, 1]
    dataset_image = dataset_image.astype(np.float32) / 255.0

    num_classes = len(np.unique(dataset_label))
    print(f'Number of classes: {num_classes}')

    # Ensure labels are in the correct shape (1-dimensional)
    dataset_label = dataset_label.flatten()
    print(f'Labels shape: {dataset_label.shape} | Image shape: {dataset_image[0].shape}')

    # Generate data for each client
    X, y, statistic = separate_data(
        (dataset_image, dataset_label), num_clients, num_classes,
        niid, balance, partition, class_per_client=2
    )
    
    # Split the data into training and testing sets
    train_data, test_data = split_data(X, y)
    
    # Save the data to files
    save_file(
        config_path, train_path, test_path, train_data, test_data, 
        num_clients, num_classes, statistic, niid, balance, partition
    )

def get_dataset_class(dataset_name):
    # Map dataset names to dataset classes
    dataset_map = {
        "pathmnist": PathMNIST,
        "chestmnist": ChestMNIST,
        "dermamnist": DermaMNIST,
        "octmnist": OCTMNIST,
        "pneumoniamnist": PneumoniaMNIST,
        "retinamnist": RetinaMNIST,
        "breastmnist": BreastMNIST,
        "bloodmnist": BloodMNIST,
        "tissuemnist": TissueMNIST,
        "organamnist": OrganAMNIST,
        "organcmnist": OrganCMNIST,
        "organsmnist": OrganSMNIST
    }
    
    # Get the dataset class from the dataset_map
    if dataset_name in dataset_map:
        return dataset_map[dataset_name]
    else:
        print(f"Unknown dataset: {dataset_name}")
        return None

if __name__ == "__main__":
    # Get user input
    dataset_name = input("Enter the dataset name (e.g., 'pathmnist', 'bloodmnist'): ").strip().lower()
    dataset_class = get_dataset_class(dataset_name)
    
    if dataset_class is None:
        exit(1)
    
    dir_path = input("Enter the directory path to save the data: ").strip()
    num_clients = int(input("Enter the number of clients: ").strip())
    niid_input = input("Enter 'noniid' for non-IID or 'iid' for IID data: ").strip().lower()
    niid = niid_input == "noniid"
    balance_input = input("Enter 'balance' for balanced data or 'unbalance' for unbalanced data: ").strip().lower()
    balance = balance_input == "balance"
    partition = input("Enter the partition method (or '-' for none): ").strip()
    partition = None if partition == "-" else partition

    # Generate the dataset
    generate_dataset(dataset_class, dir_path, num_clients, niid, balance, partition)
